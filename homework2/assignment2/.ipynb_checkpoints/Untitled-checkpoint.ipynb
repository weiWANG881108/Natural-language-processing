{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from utils.general_utils import logged_loop, get_minibatches\n",
    "from q2_parser_transitions import PartialParse, minibatch_parse\n",
    "\n",
    "P_PREFIX = '<p>:'\n",
    "L_PREFIX = '<l>:'\n",
    "UNK = '<UNK>'\n",
    "NULL = '<NULL>'\n",
    "ROOT = '<ROOT>'\n",
    "\n",
    "class Config(object):\n",
    "    language = 'english'\n",
    "    with_punct = True\n",
    "    unlabeled = True\n",
    "    lowercase = True\n",
    "    use_pos = True\n",
    "    use_dep = True\n",
    "    use_dep = use_dep and (not unlabeled)\n",
    "    data_path = './data'\n",
    "    train_file = 'train.conll'\n",
    "    dev_file = 'dev.conll'\n",
    "    test_file = 'test.conll'\n",
    "    embedding_file = './data/en-cw.txt'\n",
    "\n",
    "def read_conll(in_file, lowercase=False, max_example=None):\n",
    "    examples = []\n",
    "    with open(in_file) as f:\n",
    "        word, pos, head, label = [], [], [], []\n",
    "        for line in f.readlines():\n",
    "            sp=line.strip().split('\\t')\n",
    "            if len(sp) == 10:\n",
    "                if '-' not in sp[0]:\n",
    "                    word.append(sp[1].lower() if lowercase else sp[1])\n",
    "                    pos.append(sp[4])\n",
    "                    head.append(int(sp[6]))\n",
    "                    label.append(sp[7])\n",
    "            elif len(word) > 0:\n",
    "                examples.append({'word':word, 'pos': pos, 'head':head, 'label':label})\n",
    "                word, pos, head, label = [], [], [], []\n",
    "                if(max_example is not None) and (len(examples) == max_example):\n",
    "                    break\n",
    "        if len(word) > 0:\n",
    "            examples.append({'word':word, 'pos': pos, 'head':head, 'label':label})\n",
    "    return examples\n",
    "\n",
    "def build_dict(keys, n_max=None, offset=0):\n",
    "    print offset\n",
    "    count = Counter()\n",
    "    for key in keys:\n",
    "        count[key] += 1\n",
    "    ls = count.most_common() if n_max is None else count.most_common(n_max)\n",
    "    return {w[0]: index+offset for (index, w) in enumerate(ls)}\n",
    "    \n",
    "\n",
    "class Parser(object):\n",
    "    \"\"\"Contains everything needed for transition-based dependency parsing except for the model\"\"\"\n",
    "    \n",
    "    def __init__(self,dataset):\n",
    "        root_labels = list([l for ex in dataset for (h,l) in zip(ex['head'], ex['label']) if h==0])\n",
    "        counter = Counter(root_labels)\n",
    "        if len(counter) > 1:\n",
    "            logging.info('Warning: more than one root label')\n",
    "            logging.info(counter)\n",
    "        self.root_label = counter.most_common()[0][0]\n",
    "        deprel = [self.root_label] + list(set([w for ex in dataset for w in ex['label'] if w !=self.root_label]))\n",
    "        tok2id = {L_PREFIX+l:i for (i,l) in enumerate(deprel)}\n",
    "        tok2id[L_PREFIX + NULL] = self.L_NULL = len(tok2id)\n",
    "        \n",
    "        config = Config()\n",
    "        self.unlabeled = config.unlabeled\n",
    "        self.with_punct = config.with_punct\n",
    "        self.use_pos = config.use_pos\n",
    "        self.use_dep = config.use_dep\n",
    "        self.language = config.language\n",
    "        \n",
    "        if self.unlabeled:\n",
    "            trans = ['L', 'R', 'S']\n",
    "            self.n_deprel = 1\n",
    "        else:\n",
    "            trans = ['L-'+l for l in deprel] + ['R-'+l for l in deprel] + ['S']\n",
    "            self.n_deprel = len(deprel)\n",
    "            \n",
    "        self.n_trans = len(trans)\n",
    "        self.tran2id = {t:i for (i,t) in enumerate(trans)}\n",
    "        self.id2tran = {i:t for (i,t) in enumerate(trans)}\n",
    "        tok2id.update(build_dict([P_PREFIX+w for ex in dataset for w in ex['pos']],offset=len(tok2id)))\n",
    "        tok2id[P_PREFIX + UNK] = self.P_UNK = len(tok2id)\n",
    "        tok2id[P_PREFIX + NULL] = self.P_NULL = len(tok2id)\n",
    "        tok2id[P_PREFIX + ROOT] = self.P_ROOT = len(tok2id)\n",
    "        \n",
    "        tok2id.update(build_dict([w for ex in dataset for w in ex['word']],offset=len(tok2id)))\n",
    "        tok2id[UNK] = self.UNK = len(tok2id)\n",
    "        tok2id[NULL] = self.NULL = len(tok2id)\n",
    "        tok2id[ROOT] = self.ROOT = len(tok2id)\n",
    "        \n",
    "        self.tok2id = tok2id\n",
    "        self.id2tok = {v:k for (k,v) in tok2id.items()}\n",
    "        self.n_features = 18 + (18 if config.use_pos else 0) + (12 if config.use_dep else 0)\n",
    "        self.n_tokens = len(tok2id)\n",
    "    \n",
    "    def vectorize(self, examples):\n",
    "        vec_examples = []\n",
    "        for ex in examples:\n",
    "            word = [self.ROOT] + [self.tok2id[w] if w in self.tok2id else self.UNK for w in ex['word'] ]\n",
    "            pos = [self.P_ROOT] + [self.tok2id[P_PREFIX+w] if P_PREFIX+w in self.tok2id else self.P_UNK for w in ex['pos']]\n",
    "            head = [-1] + ex['head']\n",
    "            label= [-1] +[self.tok2id[L_PREFIX + w] if L_PREFIX + w in self.tok2id else -1 for w in ex['label']]\n",
    "            vec_examples.append({'word': word, 'pos': pos,'head': head, 'label': label})\n",
    "        \n",
    "        return vec_examples\n",
    "    \n",
    "        \n",
    "    def create_instances(self,examples):\n",
    "        all_instances = []\n",
    "        succ = 0\n",
    "        for id,ex in enumerate(logged_loop(examples)):\n",
    "            n_words = len(ex['word']) -1\n",
    "            \n",
    "            #arcs = {(h,t,label)}\n",
    "            stack = [0]\n",
    "            buf = [i+1 for i in xrange(n_words)]\n",
    "            arcs = []\n",
    "            instances = []\n",
    "\n",
    "                          \n",
    "print 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 3.68 seconds\n",
      "building parser... 6\n",
      "12\n",
      "took 0.00 seconds\n",
      "Loading pretrained embedding... took 4.04 seconds\n",
      "Vectorizing data... took 0.00 seconds\n",
      "[{'head': [-1, 2, 3, 0, 3, 3], 'word': [19, 12, 15, 14, 16, 13], 'pos': [11, 6, 6, 7, 6, 8], 'label': [-1, 4, 2, 0, 3, 1]}]\n",
      "Preprocessing training data...\n"
     ]
    }
   ],
   "source": [
    "reduced = True\n",
    "config=Config()\n",
    "start = time.time()\n",
    "train_set = read_conll(os.path.join(config.data_path, config.train_file),lowercase=config.lowercase)\n",
    "#dev_set=read_conll(os.path.join(config.data_path, config.dev_file), lowercase=config.lowercase)\n",
    "#test_set=read_conll(os.path.join(config.data_path, config.test_file), lowercase=config.lowercase)\n",
    "\n",
    "if reduced:\n",
    "    train_set=train_set[1:2]\n",
    "#    dev_set=dev_set[:500]\n",
    "#    test_set=test_set[:500]\n",
    "'''\n",
    "example \n",
    "the first example in train_set:\n",
    "{'head': [5, 5, 5, 5, 45, 9, 9, 9, 5, 9, 15, 15, 12, 15, 9, 20, 20, 19, 20, 5, 22, 20, 25, 25, 20, 20, 20, 20, 28, 28, 20, 45, 34, 45, 36, 34, 34, 34, 41, 41, 38, 34, 45, 45, 0, 48, 48, 45, 45], \n",
    "'word': ['in', 'an', 'oct.', '19', 'review', 'of', '``', 'the', 'misanthrope', \"''\", 'at', 'chicago', \"'s\", 'goodman', 'theatre', '-lrb-', '``', 'revitalized', 'classics', 'take', 'the', 'stage', 'in', 'windy', 'city', ',', \"''\", 'leisure', '&', 'arts', '-rrb-', ',', 'the', 'role', 'of', 'celimene', ',', 'played', 'by', 'kim', 'cattrall', ',', 'was', 'mistakenly', 'attributed', 'to', 'christina', 'haag', '.'], \n",
    "'pos': ['IN', 'DT', 'NNP', 'CD', 'NN', 'IN', '``', 'DT', 'NN', \"''\", 'IN', 'NNP', 'POS', 'NNP', 'NNP', '-LRB-', '``', 'VBN', 'NNS', 'VB', 'DT', 'NN', 'IN', 'NNP', 'NNP', ',', \"''\", 'NN', 'CC', 'NNS', '-RRB-', ',', 'DT', 'NN', 'IN', 'NNP', ',', 'VBN', 'IN', 'NNP', 'NNP', ',', 'VBD', 'RB', 'VBN', 'TO', 'NNP', 'NNP', '.'], \n",
    "'label': ['case', 'det', 'compound', 'nummod', 'nmod', 'case', 'punct', 'det', 'nmod', 'punct', 'case', 'nmod:poss', 'case', 'compound', 'nmod', 'punct', 'punct', 'amod', 'nsubj', 'dep', 'det', 'dobj', 'case', 'compound', 'nmod', 'punct', 'punct', 'dep', 'cc', 'conj', 'punct', 'punct', 'det', 'nsubjpass', 'case', 'nmod', 'punct', 'acl', 'case', 'compound', 'nmod', 'punct', 'auxpass', 'advmod', 'root', 'case', 'compound', 'nmod', 'punct']}\n",
    "'''\n",
    "print \"took {:.2f} seconds\".format(time.time()-start)\n",
    "\n",
    "print \"building parser...\",\n",
    "start=time.time()\n",
    "parser=Parser(train_set)\n",
    "print \"took {:.2f} seconds\".format(time.time()-start)\n",
    "\n",
    "print \"Loading pretrained embedding...\",\n",
    "start = time.time()\n",
    "word_vectors = {}\n",
    "for line in open(config.embedding_file).readlines():\n",
    "    sp = line.strip().split()\n",
    "    word_vectors[sp[0]] = [float(x) for x in sp[1:]]\n",
    "embeddings_matrix = np.asarray(np.random.normal(0,0.9,(parser.n_tokens,50)),dtype='float32')\n",
    "\n",
    "for token in parser.tok2id:\n",
    "    i = parser.tok2id[token]\n",
    "    if token in word_vectors:\n",
    "        embeddings_matrix[i] = word_vectors[token]\n",
    "    elif token.lower() in word_vectors:\n",
    "        embeddings_matrix[i] = word_vectors[token.lower()]\n",
    "\n",
    "print \"took {:.2f} seconds\".format(time.time() - start)   \n",
    "\n",
    "print \"Vectorizing data...\",\n",
    "start = time.time()\n",
    "train_set = parser.vectorize(train_set)\n",
    "#dev_set = parser.vectorize(dev_set)\n",
    "#test_set = parser.vectorize(test_set)\n",
    "print \"took {:.2f} seconds\".format(time.time() - start)\n",
    "\n",
    "'''\n",
    "the first example become\n",
    "{'head': [-1, 5, 5, 5, 5, 45, 9, 9, 9, 5, 9, 15, 15, 12, 15, 9, 20, 20, 19, 20, 5, 22, 20, 25, 25, 20, 20, 20, 20, 28, 28, 20, 45, 34, 45, 36, 34, 34, 34, 41, 41, 38, 34, 45, 45, 0, 48, 48, 45, 45], \n",
    "'word': [5156, 91, 113, 948, 600, 708, 88, 96, 85, 3417, 97, 109, 1285, 93, 3592, 3245, 145, 96, 4873, 4311, 375, 85, 5042, 91, 4401, 1625, 86, 97, 2553, 201, 3382, 144, 86, 85, 846, 88, 3152, 86, 836, 105, 2690, 3396, 86, 103, 1793, 1673, 89, 3510, 1729, 87], \n",
    "'pos': [84, 40, 41, 42, 49, 39, 40, 61, 41, 39, 62, 40, 42, 60, 42, 42, 71, 61, 53, 44, 50, 41, 39, 40, 42, 42, 45, 62, 39, 51, 44, 72, 45, 41, 39, 40, 42, 45, 53, 40, 42, 42, 45, 48, 47, 53, 52, 42, 42, 46], \n",
    "'label': [-1, 27, 31, 24, 21, 32, 27, 23, 31, 32, 23, 27, 26, 27, 24, 32, 23, 23, 33, 19, 29, 31, 9, 27, 24, 32, 23, 23, 29, 1, 8, 23, 23, 31, 6, 27, 32, 23, 37, 27, 24, 32, 23, 13, 22, 0, 27, 24, 32, 23]}\n",
    "'''\n",
    "print \"Preprocessing training data...\"\n",
    "train_examples = parser.create_instances(train_set)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
